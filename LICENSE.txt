Development Guide - Azure Functions

Azure Functions is a serverless platform that lets you run your code in response to events, without having to worry about infrastructure, scaling, or provisioning. You can use Azure Functions to create web applications, APIs, workflows, integrations, and more.

In this guide, you will learn how we at Contica develop and use Azure Functions with C#.

Contents
Structure
Setting up your solution
Folder Structure
Naming
Naming in general
Functions & Methods
Classes
Properties & Fields
Parameters & Variables
Techniques
Accessing other Azure resources
Principle Of Least Privilege
How to apply PoLP in your functions.
System Assigned Managed Identity
User Assigned Managed Identity
Shared Access Signature (SAS) Authentication
Obtaining the Connection String
Storing the Connection String
Using SAS in Your Code
Security Considerations
Working with Objects
Using Classes
Using Records
Serialization & Deserialization
Deserializing JSON or XML into objects
Serializing objects into JSON or XML
Annotations
Class Names
List and Arrays
Nullables
Wrapper Class
Error Handling
Understanding Error Types
Logging Errors
Handling Errors
Handling Transient Errors
Handling Permanent Errors:
Handling Expected Errors
Logging
Logging with Nodinite
Configure the Nodinite settings
Create the Nodinite logger
Stay updated with the logging
SOLID Principles
Single Responsibility Principle
Why Single Responsibility Principle?
Applying SRP to Azure Functions
Open-Closed Principle
Applying OCP to Azure Functions
Conclusion
Liskov Substitution Principle (LSP)
Applying LSP to Azure Functions
Interface Segregation Principle
What is Interface Segregation Principle?
Why Interface Segregation Principle?
Ways of violating ISP
Best practices
Implementation in C#​
Conclusion
Dependency Inversion Principle
What is Dependency Inversion Principle?
Why Dependency Inversion Principle?
Implementation of Dependency Inversion Principle
What is Dependency Injection?
Implementation of Dependency Injection
Implementing the Dependency Inversion Principle in C#​
Implementing Dependency Injection in C#​
Service lifetimes
Conclusion
OBS
Structure
Setting up your solution
To use Azure Functions, you need to set up a solution in Visual Studio that contains one or more projects for your functions. Each project represents a function app, which is a collection of functions that share the same configuration and resources. You can create a function app project using one of the following tools:

Visual Studio
Visual Studio Code
Command line
You can also use the Azure portal or the Azure Functions Core Tools to create and manage your function apps.

You also need to create a startup class that inherits from the FunctionsStartup abstract class. This class will be used to register and configure the services that you want to inject into your functions. You can use the built-in methods of the service collection, such as AddSingleton, AddScoped, or AddTransient, to specify the service lifetime and implementation.

You can read more about how to do this in our chapter about Dependecy Injection

Folder Structure
Within the project, files should be divided into folders and subfolders.
Example structure:

/Classes (Models)
    /Bookings
        IncomingBooking.cs
        OutgoingBooking.cs
    /Customers
        Customer.cs
        CustomerUpdate.cs
/Functions
    /Startup.cs
    /MapBookingToXML.cs
    /GetCustomer.cs
/Interfaces
    IBooking.cs
    ICustomer.cs
/Helpers
    BookingHelpers
    CustomerHelpers
/Mappers
    MapIncomingBookingToXML.cs
    MapCustomerUpdate.cs
/Utilities
    HttpClientUtils.cs
    ServiceBusUtils.cs
/Tests
    CustomerTests.cs
The exact structure can vary based on the needs of your project.
The key is to keep it organized and maintainable.

Classes: This folder contains data models that encapsulate the data your functions handle. These classes, often referred to as Models, represent specific entities in your application, such as a customer or a booking.

Functions: This folder houses your Azure functions or methods. Each function performs a specific action, such as mapping a booking to XML or retrieving customer data, and is named accordingly to reflect its purpose.

Interfaces: This folder includes interfaces that define contracts for classes.

Helpers: This folder contains helper classes that provide common functionality related to specific classes. These helpers can include methods that perform operations commonly used in conjunction with the class they’re helping, enhancing code reusability.

Mappers: This folder holds classes that handle the conversion of one data model to another.

Utilities: This folder stores utility classes that provide common, reusable methods not tied to any specific class. These utilities can include things like creating an HTTP client or establishing a service bus connection, which are operations that might be needed across multiple functions or classes.

Tests: This folder is for unit tests that verify the correctness of your code.

Naming
We try to follow the C# identifier naming rules and conventions  and .NET Naming guidelines 

Naming in general
In short we use PascalCasing when naming:

Folders
Namespaces
Interfaces (Prefix with "I" Ex: "ILogger")
Classes
Methods
Public Fields
Properties
Events
Enums
Exceptions
And we use camelCasing when naming:

Parameters
Variables
Private Fields (Prefix with "_" Ex: "_logger")
Functions & Methods
When naming Azure functions or methods use descriptive and meaningful names that convey the purpose of the function or method.
Follow a verb-noun pattern to indicate the action performed by the function.
For example: "MapThisToThat", "GetOrders", "CalculateWeight" etc.

Classes
When naming classes, use nouns based on what they represent.
For example: "Customer", "TransactionEvent", "IncomingBooking" etc.

Properties & Fields
For properties and fields, use descriptive names that reflect their purpose in the code, favoring readability over brevity. For example: "CanScrollHorizontally" is better than "ScrollableX".
In cases where you’re working with incoming data, such as JSON or XML, it’s beneficial to have your properties mirror the incoming property names. This makes mapping between the data and your code easier and more intuitive. If the incoming property names don’t follow the PascalCasing convention, you can use annotations to map the JSON/XML property to your differently named C# property.

Parameters & Variables
For parameters and variables, use descriptive names that reflect their purpose in the code.
Follow camelCasing and avoid abbreviations for clarity.
For example: "string customerName", "int orderCount", "bool isPaymentComplete" etc.

Techniques
Accessing other Azure resources
When accessing other Azure resources from within the Function code, it’s important to prioritize the authentication methods for security and manageability. Here’s a recommended priority list:

System Assigned Managed Identity: This should be your first choice for authentication. It’s secure and directly tied to your service instance. When the service instance is deleted, Azure automatically cleans up the credentials.

User Assigned Managed Identity: This is the second option to consider. It’s a standalone Azure resource that can be assigned to one or more instances of an Azure service. This offers more flexibility than system-assigned managed identities.

SAS Key Authentication: This should be your last resort. While SAS keys can provide specific access to resources, they can be more difficult to manage and rotate. They also represent a security risk if they’re compromised.

Principle Of Least Privilege
The Principle of Least Privilege (PoLP) is an information security concept where a user is given the minimum levels of access – or permissions – needed to perform their job functions.
This principle is a core concept of Zero Trust security.

How to apply PoLP in your functions.
Identify your functions roles and responsibilities:
What resources does your function use, and at what scope?

Function-Level Permissions:
Assign permissions at the function level, not at the application level. This means each function should only have the permissions it needs to perform its task.

Assign Minimal Access:
Grant each function only the access they need to perform their job functions. Does your function need access to a Service Bus? Grant access to the specific queue/topic, and only what is needed. (Reader/Sender).
Using a value stored in Key Vault, grant Key Vault Secrets User to the specific secret, not the entire Key Vault etc.

Regularly Review and Update Access Privileges:
Regularly review function privileges and adjust them as necessary. This helps prevent privilege creep, where functions accumulate more access permissions than they need.

Implement Just-in-Time Access:
If possible, grant access to resources only when it’s needed and for the shortest time necessary. This reduces the window of opportunity for a security breach.

Use Role-Based Access Control (RBAC ):
Implement Azure RBAC to manage who has access to Azure resources. RBAC provides detailed access management, allowing you to assign specific permissions to your functions based on their role.

System Assigned Managed Identity
System assigned identity is a feature of Azure that allows you to assign an identity to your Azure service, such as a function app, a web app, or a virtual machine. This identity can then be used to authenticate and authorize your code to access other Azure resources, such as storage accounts, databases, or key vaults, without having to manage any credentials or secrets in your code. You can enable a system assigned identity directly on the Azure resource, and it will be automatically deleted when the resource is deleted.

To use a system managed identity within your function code, you should use the DefaultAzureCredential Class 
from the Azure.Identity Namespace .

Alternatively, you can use the ManagedIdentityCredential Class .
The key difference between these two is:

ManagedIdentityCredential
The ManagedIdentityCredential class only looks for user assigned and system assigned identities, based on a client_id parameter. When the client_id parameter is omitted, the ManagedIdentityCredential class automatically uses the identity that is assigned (system assigned) to the Azure service where your code is running.
DefaultAzureCredential()
Tries to return credentials based on a list  of different sources, including Visual Studio Credentials. This allows the code to run locally using your own credentials, provided you're signed in and have access permission.
When your code runs in Azure, both the DefaultAzureCredential and the ManagedIdentityCredential classes return an authentication token based on the running app’s ID. If you’ve assigned the correct roles to the app in the Access Control (IAM) tab, the code running in Azure will have the same access permissions.

Here is an example using DefaultAzureCredential class to create a service bus sender:

var client = new ServiceBusClient(sbusNameSpace, new DefaultAzureCredential());
var sender = client.CreateSender(queueOrTopicName);
Here is an example using ManagedIdentityCredential class to create a service bus sender:

var client = new ServiceBusClient(sbusNameSpace, new ManagedIdentityCredential());
var sender = client.CreateSender(queueOrTopicName);
As you can see, in this case they work the same, the difference is only that the DefaultAzureCredential class is more flexible and convenient, as it can automatically choose the best credential source for your environment, and it can also work with other types of credentials besides managed identities.

User Assigned Managed Identity
User assigned identity is a feature of Azure that allows you to create a managed identity as a standalone Azure resource. You can assign this identity to one or more Azure services.

To use the ManagedIdentityCredential class with a user assigned managed identity, you need to specify the client_id of the managed identity that has been assigned to your Azure service. You can find the client_id in the overview tab of your Managed Identity resource in the Azure portal.

Here is an example using the ManagedIdentityCredential class to create a service bus sender:

var credential = new ManagedIdentityCredential(client_id: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx");
var client = new ServiceBusClient(sbusNameSpace, credential);
var sender = client.CreateSender(queueOrTopicName);
To use the DefaultAzureCredential class to do the same you simply do the same as with system assigned managed identity.

var client = new ServiceBusClient(sbusNameSpace, new DefaultAzureCredential());
var sender = client.CreateSender(queueOrTopicName);
However, if you have a resource with both a system-assigned and a user-assigned identity, and you run the DefaultAzureCredential method without specifying any options, it will by default use the system assigned identity. To bypass this behavior you need to pass the client_id of the user-assigned identity as a parameter to the ManagedIdentityCredential type. You can do this by passing a DefaultAzureCredentialOptions object to the constructor of the DefaultAzureCredential class, and setting the ManagedIdentityClientId property to the client_id of the user-assigned identity.

var options = new DefaultAzureCredentialOptions
{
      ManagedIdentityClientId = "<your-user-assigned-identity-client-id>"
};
var credential = new DefaultAzureCredential(options);
var client = new ServiceBusCluent(sbusNameSpace, credential);
var sender = client.CreateSender(queueOrTopicName);
Shared Access Signature (SAS) Authentication
A Shared Access Signature (SAS) is a URI that grants restricted access rights to Azure resources. You can use SAS to delegate access to resources in your storage account, Azure Service Bus, and Azure Event Hubs without sharing your account keys.

Obtaining the Connection String
To use SAS within your function code, you should first obtain the connection string from your Azure resource. This includes the SharedAccessKeyName and SharedAccessKey. This can be done on the “Shared access policies” tab of your resource. It’s important to note that you should not use RootManageSharedAccessKey. Instead, create a new SAS Policy and follow the Principle of Least Privilege (PoLP) guidelines.

Storing the Connection String
If possible, the connection string should be stored inside a key vault and referenced from application settings.
If you have it stored in application settings, it can be retrieved by using the Environment.GetEnvironmentVariable() method within your code.

Using SAS in Your Code
Once you have the connection string, you can use it to authenticate your Azure services. Here is an example of how to use the connection string to create a Service Bus client and then a Service Bus sender.

string connectionString = Environment.GetEnvironmentVariable("MyConnectionStringInAppSettings");
var client = new ServiceBusClient(connectionString);
var sender = client.CreateSender(queueOrTopicName);
Security Considerations
SAS should have an expiry time, after which it becomes invalid. Always transfer a SAS over HTTPS to prevent interception. Also, remember to regenerate your SAS keys periodically to reduce the risk of unauthorized access.
Read more here .

Working with Objects
At Contica we use C# objects to manage data in our Azure Functions. We avoid direct manipulation of raw data, such as JSON or XML, as much as possible. We use serialization and deserialization to convert data into C# objects and vice versa. This approach makes our code more readable, maintainable, and testable.

To work with objects, we need to create corresponding C# types that mirror the structure of the incoming or outgoing data. We can use classes or records to define our object types, depending on our needs and preferences.

We also use annotations to minimize potential serialization/deserialization errors. Annotations are attributes that provide additional information about our object types or properties. We use JSON annotations or XML annotations, depending on the data format that we work with.

Using Classes
Classes are a common way to define object types in C#. Classes are reference types that can have fields, properties, methods, constructors, and other members. Classes can also support inheritance, polymorphism, and interfaces.

We use classes when we need to encapsulate data that has complex, mutable, or interdependent state. Classes are better suited for domain models that have methods, validation, logic, or behavior associated with them.

Here is an example of a class that represents a customer:

public class Customer
{
    public string FirstName { get; set; }
    public string LastName { get; set; }
    public string Email { get; set; }

    public Customer(string firstName, string lastName, string email)
    {
        FirstName = firstName;
        LastName = lastName;
        Email = email;
    }

    public string GetFullName()
    {
        return $"{FirstName} {LastName}";
    }

    public bool IsValidEmail()
    {
        // Some validation logic here
        return true;
    }
}
This class has three properties, a constructor, and two methods.
We can create an instance of this class and use its members in our functions.

Using Records
Records are a new feature in C# 9.0 that allow us to create immutable object types that have value-based equality and concise syntax. Records are useful for representing data objects that do not change after they are created.

We use records when we need to pass or return data that is simple, immutable, and self-contained. Records are ideal for data transfer objects (DTOs) that do not have any behavior or logic associated with them.

Records are immutable by default if you use the positional syntax to define them. This means that you can use positional parameters to declare the properties of your record and generate a constructor, a deconstructor, and a ToString method automatically. For example, you can define a record type named Customer like this:

public record Customer(string FirstName, string LastName, string Email);
This record type has three positional parameters that declare the properties of the record. The properties have the init accessor, which means that they can only be set during object initialization. You cannot change the value of the properties after the object is created. For example, you can create an instance of the Customer record like this:

Customer customer = new Customer("Alice", "Smith", "alice@example.com");
But you cannot modify the properties of the customer record like this:

customer.FirstName = "Bob"; 
// Error: Init-only property or indexer 'Customer.FirstName' cannot be assigned to -- it is read only
The with keyword is a feature that allows you to create a new record instance that is a modified copy of an existing record instance. You use the object initializer syntax to specify what properties or fields to modify and their new values. For example, you can create a new record instance named customer2 that is a copy of customer with a different Email property like this:

Customer customer = new Customer("Alice", "Smith", "alice@example.com");
Customer customer2 = customer with { FirstName = "Bob" };
The with keyword performs a non-destructive mutation, which means that it does not change the original record instance, but creates a new one with the specified changes. This preserves the immutability of the record types and enables you to use them in functional programming scenarios. Read more about nondestructive mutation here 

Records are however not immutable by default if you use the nominal syntax to define them. This means that you can use the traditional way of defining classes to define your record types, with fields, properties, methods, and other members. For example, you can define a record type named Product like this:

public record Product
{
    public string Name { get; set; }
    public decimal Price { get; set; }
    public int Stock { get; set; }
}
This record type has three properties that have the set accessor, which means that they can be changed after object initialization. If you want to make your record types immutable using the nominal syntax, you need to use the init accessor instead of the set accessor for your properties.
For example:

public record Product
{
    public string Name { get; init; }
    public decimal Price { get; init; }
    public int Stock { get; init; }
}
Serialization & Deserialization
Serialization and deserialization are the processes of converting data into a format that can be stored or transmitted, and vice versa. We use serialization and deserialization to convert data into C# objects and vice versa.

We use the System.Text.Json  namespace to work with JSON data, and the System.Xml  namespace to work with XML data. These namespaces provide classes and methods that enable us to serialize and deserialize data easily and efficiently.

Deserializing JSON or XML into objects
Deserialization enables us to recreate the data and behavior of an object from a file, a database, or a network stream, back into an object’s state, and to use it in memory as needed.

To deserialize data into objects, you need to create a type that matches the structure of the incoming data.
You can use either a record or a class to define the type, depending on your preference and requirements.

Here is an example of how to deserialize JSON data for a product into a record or a class named Product:

// Define a record that represents a product
public record ProductRecord(string Name, decimal Price, int Stock);

// Or, define a class that represents a product
public class ProductClass
{
    public string Name { get; set; }
    public decimal Price { get; set; }
    public int Stock { get; set; }
}

// Create a JSON string that contains the product data
string jsonString = @"{
                       ""Name"": ""Book"",
                       ""Price"": 10.99,
                       ""Stock"": 100
                      }";

// Deserialize the JSON string into a C# objects.
Product product = JsonSerializer.Deserialize<ProductRecord>(jsonString);

Product product = JsonSerializer.Deserialize<ProductClass>(jsonString);
Here is the same example but working with incoming XML:

// Define a record that represents a product
public record ProductRecord(string Name, decimal Price, int Stock);

// Or, define a class that represents a product
public class ProductClass
{
    public string Name { get; set; }
    public decimal Price { get; set; }
    public int Stock { get; set; }
}

// Create an XML string that contains the product data
string xmlString = @"<Product>
                       <Name>Book</Name>
                       <Price>10.99</Price>
                       <Stock>100</Stock>
                     </Product>";

// Create an XmlSerializer for the type of Product
XmlSerializer recordSerializer = new XmlSerializer(typeof(ProductRecord));

XmlSerializer classSerializer = new XmlSerializer(typeof(ProductClass));

// Deserialize the XML string into a C# object of type Product
ProductRecord recordProduct;

ProductClass classProduct;

using (TextReader reader = new StringReader(xmlString))
{
    recordProduct = (ProductRecord)serializer.Deserialize(reader);

    classProduct = (ProductClass)serializer.Deserialize(reader);
}
The XmlSerializer  class can only deserialize XML data from a stream, a TextReader, or an XmlReader.
If you have XML data in a different format, such as a string or a file, you need to convert it to a stream first, and then use one of the classes mentioned above to read the stream and pass it to the Deserialize method.
Like in the example, we have XML data in a string, so we use a StringReader to wrap the string and create a TextReader, which can then be used to deserialize the XML data.

Serializing objects into JSON or XML
Serialization is the process of converting an object’s state into a format that can be stored or transmitted, and reconstructed later. Serialization enables us to save the data of an object to a file, a database, or a network stream, and to load it back into memory when needed. Serialization also allows us to share data across different applications, platforms, and languages, as long as they can understand the same serialization format.

Here is an example of how to serialize a record or a class named Product (as we defined above) into JSON data:

// Create an instance of the type of Product
ProductRecord recordProd = new ProductRecord("Book", 10.99m, 100);

ProductClass classProd = new ProductClass { Name = "Book", Price = 10.99m, Stock = 100 };

// Serialize the object into a JSON string
string recordJson = JsonSerializer.Serialize(recordProd);

string classJson = JsonSerializer.Serialize(classProd);
Here is the same example but serializing into XML data:

// Create an instance of the type of Product
ProductRecord recordProduct = new ProductRecord("Book", 10.99m, 100);

ProductClass classProduct = new ProductClass { Name = "Book", Price = 10.99m, Stock = 100 };

// Create an XmlSerializer for the type of Product
XmlSerializer recordSerializer = new XmlSerializer(typeof(ProductRecord));

XmlSerializer classSerializer = new XmlSerializer(typeof(ProductClass));

// Serialize the object into an XML string
string recordXml;``
using (StringWriter writer = new StringWriter())
{
    recordSerializer.Serialize(writer, recordProduct);
    recordXml = writer.ToString();
}

string classXml;
using (StringWriter writer = new StringWriter())
{
    classSerializer.Serialize(writer, classProduct);
    classXml = writer.ToString();
}
The XmlSerializer  class can only serialize objects to a stream, a TextWriter, or an XmlWriter. If you want to serialize objects to a different format, such as a string or a file, you need to use one of the classes mentioned above to write the serialized data to the desired destination. Like in the example, we want to serialize objects to a string, so we use a StringWriter to create a TextWriter, which can then be used to serialize the objects.

Annotations
Annotations are a way of attaching metadata to code, such as classes, methods, properties, and parameters. They can be used for various purposes, such as serialization, documentation, validation, and more.

To serialize C# objects into JSON or XML, you need to mark them with the Serializable attribute, and use the corresponding library, such as System.Text.Json  or System.Xml.Serialization . You can also customize the serialization process by using other annotations, such as JsonPropertyName, JsonIgnore, JsonInclude, XmlElement, XmlAttribute, and more.

Class Names
To annotate the class name, you can use the XmlType attribute and specify the TypeName property for XML serialization. This will override the default name of the C# class and use the custom name for the XML element. For JSON serialization, you can use a wrapper class and apply the JsonProperty attribute to the property that holds your original class. This will assign a custom name to the property in the JSON output.

List and Arrays
To annotate lists or arrays, you can use the same annotations as for regular properties, but you may need to specify the element name or type for XML serialization. For example, you can use the XmlArray and XmlArrayItem annotations to define the name and type of the array elements.

Nullables
To annotate nullable properties, you can use the same annotations as for non-nullable properties, but you may need to specify the null handling behavior for JSON serialization. For example, you can use the JsonInclude annotation to include null values in the output, or the JsonIgnoreCondition annotation to ignore null values.

For XML serialization, you can use the XmlElement annotation and specify the IsNullable property as true. This will set the isNullable attribute of the XML element to true, indicating that the element can be empty or have a null value. This can be useful for validating the XML schema or communicating the nullability of the property to other applications.

Here is an example of a sample class in C# that is annotated for both JSON and XML serialization. It contains different cases, such as nullable, ignorable, private, different data types, etc:

// Import the JSON and XML serialization libraries
using System;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Xml.Serialization;

// Mark the class as serializable
[Serializable]
// Use the XmlType attribute to specify a custom name for the class
[XmlType(TypeName = "book")]
public class Book
{
    // Use the JsonPropertyName and XmlElement attributes to specify a custom name for the property
    [JsonPropertyName("title")]
    [XmlElement("Title")]
    public string Title { get; set; }

    // Use the JsonIgnore and XmlIgnore attributes to exclude a property from serialization
    [JsonIgnore]
    [XmlIgnore]
    public int Pages { get; set; }

    // Use the JsonInclude and XmlAttribute attributes to include a non-public property in serialization
    [JsonInclude]
    [XmlAttribute("ISBN")]
    private string Isbn { get; set; }

    // Use the JsonConverter and XmlElement attributes to specify a custom converter and format for a property
    [JsonConverter(typeof(DateTimeConverter))]
    [XmlElement(DataType = "date")]
    public DateTime Published { get; set; }

    // Use the JsonConstructor and XmlElement attributes to specify a constructor to use for deserialization
    [JsonConstructor]
    [XmlElement("Book")]
    public Book(string title, string isbn, DateTime published)
    {
        Title = title;
        Isbn = isbn;
        Published = published;
    }

    // Use the XmlArray and XmlArrayItem attributes to specify the name and type of the list elements
    [XmlArray("Authors")]
    [XmlArrayItem("Author", typeof(string))]
    public List<string> Authors { get; set; }

    // Use the JsonInclude attribute to include null values in the output
    [JsonInclude]
    // Use the XmlElement attribute to specify the isNullable attribute
    [XmlElement("Rating", IsNullable = true)]
    public double? Rating { get; set; }
}
Here is the serialized JSON data of an instance of the Book class:

{
  "title": "C# in Depth",
  "ISBN": "978-0-321-87758-1",
  "Published": "2019-04-19T00:00:00",
  "Authors": [
    "Jon Skeet"
  ],
  "Rating": 4.7
}
Here is the XML output for the same instance of the Book class:

<book ISBN="978-0-321-87758-1">
  <Title>C# in Depth</Title>
  <Published>2019-04-19</Published>
  <Book>
    <Title>C# in Depth</Title>
    <ISBN>978-0-321-87758-1</ISBN>
    <Published>2019-04-19</Published>
  </Book>
  <Authors>
    <Author>Jon Skeet</Author>
  </Authors>
  <Rating isNullable="true">4.7</Rating>
</book>
Wrapper Class
A wrapper class is a class that contains another class as a field and provides access to its methods and properties. It can also add additional functionality or modify the behavior of the wrapped class. A wrapper class can be useful for various purposes, such as adding logging, caching, validation, or serialization features to an existing class.

To wrap the book class in a wrapper class, you can declare a new class and use the JsonProperty attribute to specify a custom name for the book property. This will change the JSON name of the book class to the name you provide. For example, you can write:

// Declare a wrapper class
public class Wrapper
{
    // Use the JsonProperty attribute to specify a custom name for the book property
    [JsonProperty("book_data")]
    public Book Book { get; set; }

    // Add any other methods or properties you want
}
This will produce JSON output like this:

{
  "book_data": {
    "title": "C# in Depth",
    "ISBN": "978-0-321-87758-1",
    "Published": "2019-04-19T00:00:00",
    "Authors": [
      "Jon Skeet"
    ],
    "Rating": 4.7
  }
}
Error Handling
Error Handling in Azure Functions is an important aspect of building robust and reliable Azure Functions. Properly managing errors ensures that your functions can gracefully handle unexpected situations, provide meaningful feedback, and maintain a high level of resilience.

Understanding Error Types
Before diving into error handling strategies, it's essential to understand the different types of errors that can occur in Azure Functions:

Transient Errors:
Temporary issues that might resolve themselves after a short period (like network glitches).
Retrying the operation can often resolve transient errors.

Permanent Errors:
Irrecoverable issues that require manual intervention or code changes.
Examples include invalid configurations or missing dependencies.

Expected Errors:
Errors that are part of the normal execution flow, such as validation errors.
These errors should be anticipated and handled gracefully.

Logging Errors
Logging is a fundamental aspect of error handling. Azure Functions provide built-in logging capabilities that can be leveraged to capture and analyze errors. Here's an example of logging errors using the Azure Functions logging framework:

public static class ErrorHandlingFunction
{
    [FunctionName("HandleErrors")]
    public static async Task<IActionResult> Run(
        [HttpTrigger(AuthorizationLevel.Function, "get", "post", Route = null)] HttpRequest req,
        ILogger log)
    {
        try
        {
            // Function logic that may throw exceptions

            log.LogInformation("Function executed successfully");
            return new OkResult();
        }
        catch (Exception ex)
        {
            log.LogError(ex, "An error occurred during function execution");
            return new StatusCodeResult(500);
        }
    }
}
In this example, the log.LogError method is used to log the exception details. The function returns a 500 Internal Server Error status code to indicate a failure.

Note: The default ILogger in Azure Functions logs to Application Insights by default, allowing you to analyze and monitor function execution, including any logged errors or information. This integration with Application Insights provides valuable insights into the performance and behavior of your Azure Functions.

But since our primary logging tool is Nodinite, you can find more detailed information about logging practices specific to our applications here.

Handling Errors
Handling Transient Errors
Transient errors, often caused by temporary issues such as network glitches, temporary resource unavailability, or brief service interruptions, can be handled by implementing retry logic.
Here's an example:

public static class Function
{
    [FunctionName("HandleTransientErrors")]
    public static async Task Run(
        [QueueTrigger("myqueue-items")] string myQueueItem,
        ILogger log)
    {
        int maxRetries = 5;
        int retryCount = 0;

        while (retryCount < maxRetries)
        {
            try
            {
                // Function logic that may encounter transient errors
                // ...

                // If successful, break out of the loop
                break;
            }
            catch (TransientErrorException ex)
            {
                log.LogWarning($"Transient error occurred. Retrying... ({retryCount}/{maxRetries})");
                retryCount++;

                // Implement a delay between retries, using exponential backoff if needed
                await Task.Delay(TimeSpan.FromSeconds(Math.Pow(2, retryCount)));
            }
        }

        if (retryCount == maxRetries)
        {
            log.LogError("Max retry attempts reached. Operation failed.");
            // Handle failure or throw an exception as needed
        }
    }
}
Note:
Since our Azure Functions are commonly called from within a Logic App workflow or used as an API Management backend, it's advisable to handle retry logic at that level. Both Logic Apps and API Management offer built-in features for managing transient errors and retries, providing a more centralized and effective approach to handle such scenarios.
So, it's not recommended to implement extensive retry logic within individual functions.

Handling Permanent Errors:
Permanent errors are irrecoverable issues that require manual intervention or code changes. These errors often result from issues such as invalid configurations or missing dependencies. Handling permanent errors involves thorough validation during development, testing, and monitoring in production to ensure a robust and reliable function.

Handling Expected Errors
For errors that are part of the expected execution flow, it's essential to handle them gracefully without logging them as failures. Use proper validation and condition checks to prevent these errors from causing unexpected behavior.

[FunctionName("HandleExpectedErrors")]
public static async Task<IActionResult> Run(
    [HttpTrigger(AuthorizationLevel.Function, "get", "post", Route = null)] HttpRequest req,
    ILogger log)
{
    try
    {
        // Validate input parameters
        if (string.IsNullOrEmpty(req.Query["name"]))
        {
            log.LogWarning("Name parameter is missing");
            return new BadRequestObjectResult("Name parameter is required");
        }

        // Function logic

        return new OkResult();
    }
    catch (Exception ex)
    {
        log.LogError(ex, "An unexpected error occurred");
        return new StatusCodeResult(500);
    }
}
Note:
It's advisable to implement validation schemas and policies at the API Management or Logic App level, which calls the function. By doing so, you can minimize errors related to missing parameters or other expected issues, providing a more robust and controlled data flow into your Azure Functions. This approach centralizes the validation logic and reduces the likelihood of encountering such errors within individual functions.

Logging
Azure Functions provides a built-in logging feature that allows you to write log messages to the console, the Application Insights service, or the Azure Monitor service. Application Insights is a service that collects and analyzes data from your function app, including information your app writes to logs. Azure Monitor is a service that provides a single source for monitoring Azure resources, including your function app and Application Insights.

To use the built-in logging feature, you need to enable Application Insights integration for your function app. You can do this when you create your function app, or you can enable it later from the Azure portal. For more information, read the Azure documentation .

You can use the built-in logger like this:

public static class MyFunction
{
    [FunctionName("MyFunction")]
    public static void Run([TimerTrigger("0 */5 * * * *")]TimerInfo myTimer, ILogger log)
    {
        try
        {
            // Do some work here
            log.LogInformation("Function executed successfully.");
        }
        catch (Exception ex)
        {
            // Handle the exception here
            log.LogError(ex, "Function failed with an exception.");
        }
    }
}
Logging with Nodinite
Nodinite  is a platform that enables end-to-end monitoring and logging of distributed systems, such as Azure Functions. Nodinite can collect, store, and display log events from various sources, such as Azure Event Hubs, Azure Service Bus, or Azure Storage Queues.

The way we are using Nodinite to handle logging within our functions is to write log events to an Azure Event Hub, which Nodinite monitors.

To use Nodinite with Azure Functions, you need to install the Serilog NuGet  and Nodinite Serilog Event Hub Sink NuGet  packages and configure the Nodinite settings in your code.

Configure the Nodinite settings
To configure the Nodinite settings, you need to create an instance of the NodiniteLogEventSettings class, which is used to configure the logging information that Nodinite collects and displays from Azure Functions. Each property of the class represents a different aspect of the log event.
You can find more details about each property in the Nodinite documentation. .

Here is an example:

 NodiniteLogEventSettings settings = new NodiniteLogEventSettings()
 {
     LogAgentValueId = 15,
     EndPointDirection = endPointDirection,
     EndPointTypeId = 75,
     EndPointUri = "Nodinite.Serilog.EventHubSink",
     EndPointName = functionAppName + "/" + functionName,
     ProcessingUser = "NODINITE",
     ProcessName = "Azure.FunctionApp" + functionName,
     ProcessingMachineName = "Azure",
     ProcessingModuleName = "Azure.FunctionApp." + functionAppName,
     ProcessingModuleType = "Azure.FunctionApp"
 };
Create the Nodinite logger
To create a Logger that writes to Nodinite, we create an instance of the LoggerConfiguration class, which is used to configure a Serilog Logger.
On that we then call the WriteTo.NodiniteEventHubSink method, which is provided by the Nodinite Serilog Event Hub Sink NuGet package . This method takes the connection string and the settings as parameters and sets up the Logger to write to the Event Hub using Nodinite.

Here is an example:

LoggerConfiguration loggerConfig = new LoggerConfiguration();
loggerConfig.WriteTo.NodiniteEventHubSink(connectionString, settings);
ILogger logger = loggerConfig.CreateLogger();
We then use the logger to write log events  to the event hub.
Here is a custom implementation using the ForContext method to add a custom property called Body containing a custom NodiniteMessage object, and another property called OriginalMessageType with information about the Function App and specific function. It then uses the logger with this custom properties added, and writes an Information level log event.

Here the custom implementation:

public static void LogToNodinite(this ILogger logger, string functionName, int statusCode, string message, object payload, object[] trackedProperties = null)
{
    logger.ForContext("Body", JsonSerializer.Serialize(new NodiniteMessage()
    {
        StatusCode = statusCode,
        Message = message,
        TrackedProperties = trackedProperties,
        Payload = payload
        })).ForContext("OriginalMessageType", "Azure Function - " + functionName)
        .Information("");
}
Stay updated with the logging
The logging with Nodinite and Serilog is constantly evolving and improving.
We have a repository and NuGet package with the latest changes and enhancements.
You can find them in our repository.

SOLID Principles
S - Single Responsibility Principle
O - Open-Closed Principle
L - Liskov Substitution Principle
I - Interface Segregation Principle
D - Dependency Inversion Principle
Single Responsibility Principle
Defenition

A class should have only one reason to change, meaning that a class should only have one responsibility.

In the context of Azure Functions, which are designed to be small, single-purpose units of execution, applying the Single Responsibility Principle becomes crucial for maintaining a clean, modular, and scalable codebase.

Why Single Responsibility Principle?
Modularity and Maintenance:
Each Azure Function should be designed to perform a single, well-defined task. This makes the code modular and easy to maintain.

Scalability:
Single-purpose functions are easier to scale independently. You can scale specific functions based on demand without affecting others.

Readability and Understandability:
Functions with a single responsibility are easier to understand, leading to improved code readability.

Applying SRP to Azure Functions
Clearly Define Function's Purpose:
Each Azure Function should have a clear and specific purpose. Avoid combining multiple unrelated tasks within a single function.

Keep Functions Cohesive:
Ensure that the logic within a function is cohesive, focusing on its defined purpose. Avoid introducing unrelated functionalities.

Avoid Fat Functions:
Functions should be concise and focused. Avoid creating functions that try to do too much. If a function grows too large, consider breaking it into smaller, more focused functions.

Use Helper Functions and Classes:
If a function requires multiple steps or complex logic, consider breaking it down into smaller helper functions or even separate classes. This promotes code reuse and maintainability.

Open-Closed Principle
Definition

Software entities (classes, modules, functions, etc.) should be open for extension but closed for modification.

The Open-Closed Principle encourages the design of systems that can be easily extended with new functionality without modifying existing code. This is typically achieved through the use of abstraction, inheritance, and polymorphism.

Example:
Let's consider a simple Azure Function that processes orders:

public class OrderProcessor
{
    public void ProcessOrder(Order order)
    {
        // Process the order
        // ...
    };
}
Now, according to the Open-Closed Principle, if you need to add new functionality or support new types of orders, you should do so without modifying the existing OrderProcessor class. Instead, you extend or override its behavior.

public interface IOrderProcessor
{
    void ProcessOrder(Order order);
}

public class RegularOrderProcessor : IOrderProcessor
{
    public void ProcessOrder(Order order)
    {
        // Process regular order logic
        // ...
    };
}

public class VIPOrderProcessor : IOrderProcessor
{
    public void ProcessOrder(Order order)
    {
        // Process VIP order logic
        // ...
    };
}
Here, we've introduced an interface IOrderProcessor, and created two classes RegularOrderProcessor and VIPOrderProcessor that implement this interface. The OrderProcessor class can now use any class that implements IOrderProcessor without modification.

Applying OCP to Azure Functions
In the context of Azure Functions, especially small, single-purpose functions, the Open-Closed Principle can be applied as follows:

Separate Concerns:
Each Azure Function should have a specific purpose or concern. If you need to extend the functionality, create a new function rather than modifying existing ones.

Use Interfaces or Abstract Classes:
Define interfaces or abstract classes to represent the contracts for different types of functions. This allows you to introduce new functionality without changing the existing implementation.

Leverage Dependency Injection:
Use dependency injection to inject dependencies into your Azure Functions. This enables you to replace or extend functionality without modifying the function itself.

Example in Azure Functions:

public interface IOrderProcessor
{
    void ProcessOrder(Order order);
}

public class RegularOrderProcessor : IOrderProcessor
{
    public void ProcessOrder(Order order)
    {
        // Process regular order logic
        // ...
    };
}

public class VIPOrderProcessor : IOrderProcessor
{
    public void ProcessOrder(Order order)
    {
        // Process VIP order logic
        // ...
    }
}
public class OrderFunction
{
    private readonly IOrderProcessor _orderProcessor;

    public OrderFunction(IOrderProcessor orderProcessor)
    {
        _orderProcessor = orderProcessor;
    }

    [FunctionName("ProcessRegularOrder")]
    public void ProcessRegularOrder([QueueTrigger("regular-orders")] Order order)
    {
        _orderProcessor.ProcessOrder(order);
    }

    [FunctionName("ProcessVIPOrder")]
    public void ProcessVIPOrder([QueueTrigger("vip-orders")] Order order)
    {
        _orderProcessor.ProcessOrder(order); 
    }
}
Here, the OrderFunction class contains two Azure Functions (ProcessRegularOrder and ProcessVIPOrder) that are triggered by Azure Queue messages. The specific order processing logic is delegated to the IOrderProcessor implementation, which we have injected with constructor injection.

This design allows you to introduce new types of orders or modify the processing logic without changing the existing functions.

Conclusion
The Open-Closed Principle encourages a design that facilitates extensibility without modifying existing code. In the context of Azure Functions, this principle helps create modular and maintainable functions, especially when dealing with small, single-purpose functions.

Liskov Substitution Principle (LSP)
Definition

Subtypes must be substitutable for their base types without altering the correctness of the program.

In simpler terms, if a class is a subtype of another class, you should be able to use objects of the subtype wherever objects of the base type are used without affecting the correctness of the program.

Example:
Consider a scenario where you have a base class Bird with a method Fly:

public class Bird
{
    public virtual void Fly()
    {
        Console.WriteLine("Bird is flying");
    };
}
Now, you create a subtype Penguin that inherits from Bird:

public class Penguin : Bird
{
    // Penguins can't fly, so we override the Fly method
    public override void Fly()
    {
        Console.WriteLine("Penguin can't fly");
    };
}
The Penguin class overrides the Fly method, indicating that penguins cannot fly.

Now, according to the Liskov Substitution Principle, you should be able to use an instance of Penguin wherever you use an instance of Bird. For example:

Bird bird = new Penguin();
bird.Fly(); // Should print "Penguin can't fly"
Applying LSP to Azure Functions
In the world of small Azure Functions, we don't need to overly worry about the Liskov Substitution Principle.
It's a bigger deal in complex setups. For our simple functions, think of it like using specialized tools for specific tasks, keeping our approach practical and focused.

However, it's still worth keeping the following considerations in mind:

Avoid Breaking Existing Contracts:
If you have a function that expects a certain input type, introducing a new type should not break the existing functionality. Ensure backward compatibility.

Keep Functions Cohesive:
Each Azure Function should have a clear and specific purpose. Avoid creating complex hierarchies of functions that may introduce confusion.

Favor Composition over Inheritance:
In scenarios where you need to reuse functionality, consider using composition or interfaces instead of deep inheritance hierarchies. This aligns with SOLID principles.

Remember that the applicability of principles like LSP may vary based on the complexity of the system. In simpler scenarios, the focus may be more on readability, maintainability, and simplicity.

Interface Segregation Principle
What is Interface Segregation Principle?
The definition of ISP (Interface Segregation Principle) is basically that:

clients should not be forced to implement behaviors they don't need. Instead, interfaces should be broken down into smaller and more specific interfaces.

This prevents clients from depending on behaviors that are irrelevant to their needs.

Why Interface Segregation Principle?
One of the goals in creating a software system is to make it maintainable. Achieving this goal becomes much easier by having a decoupled program.
To have a decoupled system, we can take advantage of the ISP. This Principle helps the classes to be depended on what should be done rather than how it should be done.
The ISP makes testing much easier, allowing each behavior/interface to be tested separately without considering how other parts should be handled. More on this will be discussed later.
This also enables developers to create modular systems that are easy to extend and modify.

Ways of violating ISP
Fat interfaces: An interface with too many methods can lead to code bloat and increased coupling.
Unrelated methods: Having methods that are unrelated to its core purpose forces clients to depend on methods they don't need.
Best practices
Keep interfaces focused: Designing interfaces with a single, focused core purpose prevents clients from being forced to implement unrelated methods.
Use multiple interfaces: Using multiple, smaller interfaces with a single core purpose helps overcome the creation of a single, monolithic interface.
These multiple interfaces can then be combined as needed to suit different client requirements.
Leverage Composition: Utilize composition to combine multiple interfaces, creating flexible, modular classes/interfaces that can be easily extended and modified.
Implementation in C#​
Source code

To illustrate the ISP we have a simple example. Let's say we have an IWorker interface which defines methods for various types of work.

public interface IWorker
{
    void Work();
    void Eat();
    void Sleep();
}
Now, imagine we have two classes, HumanWorker and RobotWorker, that implements this interface.

public class HumanWorker : IWorker
{
    public void Work() { /*...*/ }
    public void Eat() { /*...*/ }
    public void Sleep() { /*...*/ }
}
public class RobotWorker : IWorker
{
    public void Work() { /*...*/ }
    public void Eat() { /*...*/ } // Not applicable to robots
    public void Sleep() { /*...*/ } // Not applicable to robots
}
This example shows a clear violation against ISP as it forces RobotWorker class to implement the methods that are irrelavant for robots. To adhere to the ISP,
we can breake down the Iworker interface into smaller and more specific interfaces. Starting with defining the behaviors:

public interface IWorkable
{
    void Work();
}

public interface IEatable
{
    void Eat();
}

public interface ISleepable
{
    void Sleep();
}
Then, we use composition by having interfaces for each IHumanWorker and IRobotWorker to have more flexibility and make it easier to extend.

public interface IHumanWorker : IWorkable, IEatable, ISleepable
{

}
public interface IRobotWorker : IWorkable
{
    void GiveFactoryInfo();
}
Now, the HumanWorker and RobotWorker can implement their own interfaces without being forced to include irrelevant behaviors.

public class HumanWorker : IHumanWorker
{
    public void Work(){ /*...*/ }

    public void Eat(){ /*...*/ }

    public void Sleep(){ /*...*/ }
}
public class RobotWorker : IRobotWorker
{
    public void Work(){ /*...*/ }

    public void GiveFactoryInfo(){ /*...*/ }
}
Conclusion
By understanding the ISP and adhering to its principles, developers can create cleaner and more flexible codebases that are easier to maintain, extend, modify, test, and scale.
The ISP also aids in following the single responsibility principle.

Dependency Inversion Principle
What is Dependency Inversion Principle?
One fundamental principle of the SOLID principles is the Dependency Inversion Principle (DIP).

The essence of DIP is that higher-level modules should not rely on lower-level modules; rather, both should depend on abstractions.

Following the DIP allows developers to decouple the higher-level components of their applications from low-level implementation details.
This results in code that is not only more flexible for modifications but also easier to test and extend.

Why Dependency Inversion Principle?
Using the DIP results several benifits, such as:

Decoupling: The DIP helps to create a more modular code that is easier to maintain and extend.
Testability: The DIP can make the code decoupeld and a decoupeld code is easier to test, as each component/module can be tested independently.
Reusability: By creating modules that are less depended on other modules, they will become more reusable.
Implementation of Dependency Inversion Principle
To reduce direct dependencies between high-level and low-level components, prioritize creating abstractions such as interfaces and abstract classes.
By having both high-level and low-level components depend on these abstractions, you can achieve the goal of minimizing direct dependencies.

What is Dependency Injection?
Dependency Injection (DI) is a technique employed to supply classes with their dependencies from external sources.
It is frequently utilized in conjunction with the Dependency Inversion Principle to enhance decoupling and facilitate better testability.

Implementation of Dependency Injection
There are several ways to implement DI in C#:

Constructor Injection: Injecting dependencies through the constructor of a class.
Property Injection: Injecting dependencies through public properties of a class.
Method Injection: Injecting dependencies as parameters of a method.
Implementing the Dependency Inversion Principle in C#​
Source code

Let's walk through a simple example of how it could be implemented in your C# program.

Without applying the DIP, our program might look like this:

public class Employ
{
    public string Name { get; set; }

    public int AccountNumber { get; set; }
}
public class Klarna
{
    public void Pay(int accountNumber, double amount) { /*...*/ }
}
public class EconomyDepartment
{
    private readonly Klarna _klarna;

    public EconomyDepartment()
    {
        _klarna = new Klarna();
    }

    public void PayToEmploy(Employ employ, double amount)
    {
        _klarna.Pay(employ.AccountNumber, amount);
    }
}
In this scenario, there is a high dependency on the Klarna class. The issue arises if, in the future, you decide to change the payment method.
This would necessitate modifying the EconomyDepartment, which is not an ideal situation.

To lessen this coupling, we introduce the DIP into our program:

public interface IPaymentManager
{
    void PayToEmploy(Employ employ, double amount);
}
public class KlarnaPaymentManager: IPaymentManager
{
    private readonly Klarna _klarna;

    public PaymentManager()
    {
        _klarna = new Klarna();
    }

    public void PayToEmploy(Employ employ, double amount)
    {
        _klarna.Pay(employ.AccountNumber, amount);
    }
}
public class EconomyDepartment
{
    private readonly IPaymentManager _paymentManager;

    public EconomyDepartment(IPaymentManager paymentManager)
    {
        _paymentManager = paymentManager;
    }

    public void PayToEmploy(Employ employ, double amount)
    {
        _paymentManager.PayToEmploy(employ, amount);
    }
}
As you can see now, EconomyDepartment is not directly dependent on a specific PaymentManager. If you decide to change the payment method,
you can create a new class implementing the IPaymentManager interface and pass an instance to the constructor without altering anything in the EconomyDepartment class.

Implementing Dependency Injection in C#​
To showcase the DI technique, we will utilize the codebase from the previous section.

In order to implement DI in FunctionApps we need following nugets (ASP.NET has built in support for DI):

Microsoft.Azure.Functions.Extensions, latest version
Microsoft.Extensions.DependencyInjection, latest version
Now, let's walk through the code:

[assembly: FunctionsStartup(typeof(IDDemo.Startup))]

namespace IDDemo;

public class Startup : FunctionsStartup
{
    public override void Configure(IFunctionsHostBuilder builder)
    {
    }
}
You need to create a new class called Startup. As the name suggests, it serves as the entry point for your Function Apps. To register the method,
add the FunctionsStartup assembly attribute specifying the type name as {namespace.Startup}.

Now, you need to register the services that you're going to use in the app:

[assembly: FunctionsStartup(typeof(IDDemo.Startup))]

namespace IDDemo;

public class Startup : FunctionsStartup
{
    public override void Configure(IFunctionsHostBuilder builder)
    {
        builder.Services.AddScoped<IHumanWorker, HumanWorker>();
        builder.Services.AddScoped<IRobotWorker, RobotWorker>();

        builder.Services.AddScoped<IPaymentManager, KlarnaPaymentManager>();
        builder.Services.AddScoped<EconomyDepartment>();
    }
}
To breake down the syntax of service registeration, take a look at <IPaymentManager, KlarnaPaymentManager>. This signifies that whenever an IPaymentManager service is required,
provide the KlarnaPaymentManager as the concrete implementation of the interface. Certainly, if you want to register a service with its type directly,
you can do so using for instance builder.Services.AddScoped<EconomyDepartment>() method.

Now, the magic happens:

public class EconomyDepartment
{
    private readonly IPaymentManager _paymentManager;

    public EconomyDepartment(IPaymentManager paymentManager)
    {
        _paymentManager = paymentManager;
    }

    public void PayToEmploy(Employ employ, double amount)
    {
        _paymentManager.PayToEmploy(employ.AccountNumber, amount);
    }
}
Here, PaymentManager is injected through the constructor, and the instance is stored in a readonly field, making it possible for you to use it throughout your entire class.

Service lifetimes
Azure Functions apps provide the same service lifetimes as ASP.NET. For a Functions app, the different service lifetimes behave as follows:

Transient: Transient services are created upon each resolution of the service.
Scoped: The scoped service lifetime matches a function execution lifetime. Scoped services are created once per function execution.
Later requests for that service during the execution reuse the existing service instance.
Singleton: The singleton service lifetime matches the host lifetime and is reused across function executions on that instance.
Singleton lifetime services are recommended for connections and clients, for example DocumentClient or HttpClient instances.
Conclusion
The Dependency Inversion Principle is a powerful concept in object-oriented programming that can significantly enhance the quality of your C# code.
By adhering to this principle, you can develop code that is more maintainable, testable, and reusable.
Combining the DIP with Dependency Injection further improves the flexibility and testability of your applications.
It also provides you with a good overview and control over the services you use.

OBS
Using these principles doesn't automatically transform your application into a Microservices architecture.
While SOLID principles, Dependency Inversion Principle, and Dependency Injection contribute to building more maintainable and scalable software,
a Microservices architecture involves additional considerations such as independent deployability of services, decentralized data management, and
communication between services through APIs. Applying these principles is a step towards better software design, but the adoption of a Microservices architecture
involves broader architectural decisions.
